# I) Installation KIâ€‘Agent 1 ohne RAG (Dify + Ollama) âœ¨


### Voraussetzungen
- Docker Desktop installiert und laufend
- Git installiert
- Ollama (und die Modelle llama3.1:8b und qwen3:8b) installiert
- macOS/Windows empfohlen. Linux geht auch (siehe Dify Doku fÃ¼r Linux).
- (fÃ¼r macOS) Homebrew installiert


## Setup

### 1) Agent 1 Chat App erstellen
- In Dify: Create App -> Chat App.
- Provider: OpenAI-API-compatible wÃ¤hlen, dein Modell auswÃ¤hlen.
- Knowledge/RAG: nicht hinzufÃ¼gen bzw. deaktiviert lassen.
- System Prompt (einfÃ¼gen und bei Bedarf anpassen):
  >   - Opening message: z. B. Willkommen! Ich begleite dich Schritt fÃ¼r Schritt durch Design Thinking. Nenne mir zuerst kurz dein Problem oder Ziel. Danach starten wir mit Phase 1 â€“ Empathize.
  >   - Rolle und Ziel:
  >   - Du bist ein methodischer Moderator, der den Anwender strikt durch ein mehrstufiges Design-Thinking-Verfahren fÃ¼hrt, um eine LÃ¶sung fÃ¼r ein von ihm benanntes Problem zu entwickeln. Keine Wissensanreicherung aus externen Quellen verwenden.
  > - Vorgehen in 5 Phasen (strikt nacheinander):
  >   - Phase 1 â€“ Empathize: KlÃ¤re Zielgruppe, Kontext, BedÃ¼rfnisse, Schmerzpunkte. Sammle Kernbeobachtungen in Stichpunkten. Frage nach, bis die Infos ausreichend sind. Abschlussoutput: â€œEmpathize-Zusammenfassungâ€.
  >   - Phase 2 â€“ Define: Verdichte zu 1â€“2 klaren Problemstatements mit â€œWer? Was? Warum wichtig?â€. Lass den Anwender bestÃ¤tigen/Ã¤ndern. Abschlussoutput: â€œProblemdefinition v1â€.
  >   - Phase 3 â€“ Ideate: Erzeuge 5â€“8 LÃ¶sungsansÃ¤tze mit kurzen Vor-/Nachteilen. Bitte den Anwender, 1â€“2 Favoriten auszuwÃ¤hlen. Abschlussoutput: â€œTop-Ideenâ€.
  >   - Phase 4 â€“ Prototype: Beschreibe fÃ¼r die Top-Idee(n) einen Low-Fidelity-Prototypen (Ziele, Hauptfunktionen, grober Userflow/Screen-Skizze in Text). Abschlussoutput: â€œPrototyp-Plan v1â€.
  >   - Phase 5 â€“ Test: Formuliere 5â€“8 Testfragen/Tasks, definiere Erfolgskriterien und nÃ¤chste Schritte.
  > - Interaktionsregeln:
  >   - Stelle immer nur eine Phase zugleich dar und frage explizit â€œWeiter mit Phase X?â€.
  >   - Nutze klare Listen, kurze SÃ¤tze, deutschsprachig, inklusiver Ton.
  >   - Referenziere die Nutzerantworten korrekt und iteriere.
  > - Abschluss:
  >   - Erzeuge eine Endzusammenfassung mit: Problemstatement, Annahmen, ausgewÃ¤hlte Idee(n), Prototyp-Plan, Testplan, Risiken/NÃ¤chste Schritte.
- Publish klicken > Publish update > Run App .

### 2) Template exportieren (fÃ¼r die Weitergabe)
- In der App: Export Template -> Datei sichern (z. B. dify-template.json).

## Usage and Configuration

### Chat Format
- The AI Agent works as a Chat format, meaning every new chat is like a new conversation. You can activate the 'memory' features in the settings so that the Agent remembers things from other chats and conversations, but this uses more resources - and sometimes you might just want to start fresh every time. 

### Multiple Agents
- Dify gives you the opportunity to create multiple Agents - meaning that, if you have different use cases, you can create a dedicated Agent for each use case. You can create Agents from the main Dify page (click on the Dify logo).

### How to write prompts (Prompt engineering)
- You can change the prompt however you like. The current prompt is just an example. Your AI Agent could be a lawyer, teacher, strategist, or anything else. Just keep in mind that, the more detailed context you give it, the better and more accurate it will be. Imagine this is a person living far away that doesn't know anything about you or what you're trying to do. And you're chatting online with this person (who is an expert on many things), so you need to give them a detailed background of the problem.

### Placeholder template prompt
- If you need a placeholder template as a starter prompt, you can use the AUTOMAT framework, but there are many you can find online. Here you will find some basic ones: https://medium.com/the-generator/the-perfect-prompt-prompt-engineering-cheat-sheet-d0b9c62a2bba

### Variables
- You can add variables for the prompt. Inside the prompt text, just add two curly brackets with a variable name inbetween (`{{name}}`) and then add it as a variable in the Dify menu below the prompt text. Depending on how you included the variable inside your prompt, the Agent will use that variable accordingly. A simple example you can test is telling it to call you by your name. (`Call me by my name {{name}}`).

### Uploading documents
- It is possible to upload documents that the AI Agent will later use as an information source. This would be useful, for example, if you had a large PDF file with lots of information about a certain topic. You could upload it and ask the AI specific questions about the topic. Or summarize the whole document. Or improve the spelling and grammar. There are many use cases.
- To do this, you have to go to the Main Menu Knowledge Tab (top bar) > Create Knowledge > Import from file > Select/Drag&Drop your file > Next > High Quality > Save & Process. After you upload it there, you can go to your chat in studio view, Knowledge Dropdown on the left side (not top bar), and just select the document you uploaded earlier. Now, the AI Agent will know everything inside that document.

### Other features
- There are many other possible features that take a bit more time to set up (e.g. Vision feature to analyse uploaded Images). If you have some experience and have the extra time, you can try setting them up. 

### Limitations
- There are limitations when it comes to the amount of information the AI Agent can process. If you chat to the Agent for long enough, it will start to forget what you talked about in the beginning, and might get confused halfway through. Also, if you upload documents for the AI to use as a source of information, and the documents are too large, the AI Agent might also forget things or lose context. You can learn the exact limitations via trial and error.
- The AI Agents are not perfect and they often hallucinate. So you definitely need to double check the information you get from them, since they will confidently give you wrong answers.

## FAQ & Troubleshooting ðŸ§©

- Warum funktioniert die URL http://host.docker.internal:11434/v1? ðŸ”
  - Dify lÃ¤uft in Dockerâ€‘Containern. Damit ein Container (Dify) den Ollamaâ€‘Dienst auf deinem Hostâ€‘Rechner erreicht, gibt es host.docker.internal (auf macOS/Windows). Das ist eine spezielle Hostnameâ€‘BrÃ¼cke vom Container zum Host.
  - Der /v1â€‘Pfad: Ollama bietet eine OpenAI-kompatible API unter dem Pfad /v1. Dify erwartet genau dieses Schema. Deshalb muss die Baseâ€‘URL auf /v1 enden.
  - Linuxâ€‘Sonderfall: host.docker.internal ist dort oft nicht vordefiniert. Nimm stattdessen die Gatewayâ€‘IP der Dockerâ€‘Bridge (hÃ¤ufig 172.17.0.1). Du findest sie mit: docker network inspect bridge und suchst nach "Gateway".

- Es gibt keinen â€œChatâ€â€‘Typ bei â€œModel Typeâ€.
  - Korrekt. In Dify heiÃŸt das einfach â€œLLMâ€. Das ist der Chat/Textâ€‘Generierungstyp.

- 404 Not Found bei APIâ€‘Aufruf.
  - Meist fehlt /v1 am Ende der Baseâ€‘URL. Nutze z. B. http://host.docker.internal:11434/v1

- Verbindung fehlgeschlagen von Dify zu Ollama.
  - macOS/Windows: Nutze host.docker.internal, nicht localhost.
  - Linux: Nutze http://172.17.0.1:11434/v1 oder deinen Bridgeâ€‘Gatewayâ€‘Host (docker network inspect bridge).
  - PrÃ¼fe, ob Ollama lÃ¤uft: ollama list

- â€œModel not foundâ€ oder â€œThe model does not existâ€.
  - Der Eintrag â€œmodel name for API endpointâ€ muss exakt dem Ollamaâ€‘Tag entsprechen (z. B. llama3.1:8b). PrÃ¼fe mit ollama list oder curl http://localhost:11434/api/tags

- Was trage ich bei â€œAPI Keyâ€ ein?
  - Irgendeinen nichtâ€‘leeren String (z. B. ollama). Ollama prÃ¼ft das nicht, Dify will aber ein Feld.

- Ist .yml und .yaml dasselbe?
  - Ja. Beides ist YAML. Docker Compose akzeptiert beide. Falls der Dateiname abweicht, nutze docker compose -f datei.yaml up -d

- Responses sind langsam.
  - Kleinere Modelle nutzen (z. B. qwen2.5:7b-instruct).
  - In Dify die Maxâ€‘Token reduzieren.
  - Genug RAM/CPU sicherstellen.

- Kann ich ein anderes Modell nutzen?
  - Ja. In Ollama den passenden Tag ziehen (ollama pull ...), danach in Dify dasselbe Vorgehen: Model Name und model name for API endpoint identisch zum Tag.

- â€œlocalhostâ€ funktioniert nicht aus Dify.
  - Dify lÃ¤uft im Container. localhost meint dann den Container selbst, nicht deinen Host. Daher host.docker.internal (Mac/Win) bzw. Bridgeâ€‘Gateway (Linux).

- Linux: Wie finde ich die Gatewayâ€‘IP?
  - docker network inspect bridge und nach "Gateway" suchen. Typisch ist 172.17.0.1

- Portâ€‘Konflikte beim Start von Dify.
  - PrÃ¼fe, ob 3000 (Dify UI) oder weitere in der Compose belegte Ports frei sind. Passe Ports in der docker-compose.yaml an, falls nÃ¶tig, und starte neu.

- Ich habe versehentlich Knowledge/RAG aktiviert.
  - In der App die Knowledgeâ€‘Sektion entfernen bzw. sicherstellen, dass kein Datenspeicher verbunden ist. FÃ¼r Agent 1 brauchen wir kein RAG.

- Wie exportiere/importiere ich das Template?
  - In der App: Export Template -> Datei sichern. Import analog Ã¼ber Import Template.
