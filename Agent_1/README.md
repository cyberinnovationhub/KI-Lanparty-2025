# I) Installation Lokaler KIâ€‘Agent ohne RAG (Dify + Ollama) âœ¨

### Ziel
- In 60â€“90 Minuten einen lokalen KIâ€‘Agenten (ohne RAG) mit Dify und einem lokalen LLM (Ollama) aufsetzen.
- Keine Azure, keine Embeddings, kein RAG. Nur ein klarer, mehrstufiger Prozess (Design Thinking) mit vorgefertigtem Systemprompt.
- Am Ende kannst du die Difyâ€‘App als Template exportieren und weitergeben.

### Voraussetzungen
- Docker Desktop installiert und laufend
- Git installiert
- Ollama installiert und laufend
- macOS/Windows empfohlen. Linux geht auch (siehe Hinweise zu host.docker.internal).
- (fÃ¼r macOS) Homebrew installiert


## Schnellstart (TL;DR) ðŸŒ±

1) Ollama installieren und ein Modell ziehen:
- `ollama pull llama3.1:8b`
2) Dify Repo klonen und starten:
- `git clone https://github.com/langgenius/dify`
- In den Ordner mit der <ins>docker-compose.yaml</ins> wechseln (siehe Dify README).
- .env.example nach .env kopieren.
- `docker compose up -d`
3) Dify Ã¶ffnen: http://localhost und Admin anlegen.
4) In Dify unter Settings -> Model Providers -> OpenAI-API-compatible ein neues Modell hinzufÃ¼gen:
- Model Type: LLM
- API endpoint URL:
  - macOS/Windows: http://host.docker.internal:11434/v1
  - Linux: http://172.17.0.1:11434/v1 (oder Docker-Bridge-Gateway)
- API Key: ollama (beliebiger String)
- Model Name und model name for API endpoint: exakt der Ollamaâ€‘Tag, z. B. llama3.1:8b
5) Chat App erstellen, RAG/Knowledge aus lassen, Systemprompt einfÃ¼gen, verÃ¶ffentlichen, testen.


## Schritt fÃ¼r Schritt ðŸ€

1) Ollama installieren und Modell vorbereiten
- macOS: `brew install ollama`
- Windows: Installer von ollama.com
- Linux: curl-Install gemÃ¤ÃŸ ollama.com
- Service starten (lÃ¤uft meist automatisch nach Installation).
- Modell ziehen:
  - `ollama pull llama3.1:8b`
  - Alternativ: `ollama pull qwen2.5:7b-instruct`
- Testen:
  - `ollama list` (zeigt installierte Modelle)
  - Optional: einfacher APIâ€‘Test
    - `curl http://localhost:11434/v1/chat/completions -H "Content-Type: application/json" -H "Authorization: Bearer ollama" -d '{"model":"llama3.1:8b","messages":[{"role":"user","content":"Hallo"}]}'`

2) Dify lokal starten
- Repository klonen:
  - `git clone https://github.com/langgenius/dify`
- In den Ordner wechseln, der die docker-compose.yaml enthÃ¤lt (siehe Difyâ€‘README).
- .env anlegen:
  - Kopiere .env.example nach .env
- Start:
  - `docker compose up -d`
- UI Ã¶ffnen:
  - http://localhost:3000
- Adminâ€‘Konto anlegen.

3) Dify mit Ollama verbinden (OpenAI-API-kompatibel) ðŸ”Œ
Im Difyâ€‘UI: Settings -> Model Providers -> OpenAI-API-compatible -> Add model

Felder ausfÃ¼llen (Beispiel: llama3.1:8b):
- Model Type: LLM
- Model Name: llama3.1:8b
- Authorization Name: Authorization
- Model display name: Llama 3.1 8B (Ollama)  [frei wÃ¤hlbar]
- API Key: ollama  [beliebiger String, wird von Ollama nicht geprÃ¼ft]
- API endpoint URL:
  - macOS/Windows: http://host.docker.internal:11434/v1
  - Linux: http://172.17.0.1:11434/v1 (oder das Gateway aus docker network inspect bridge)
- model name for API endpoint: llama3.1:8b  [muss exakt zum Ollamaâ€‘Tag passen]

Alternative Modellwerte (wenn du Qwen nutzt):
- Model Name: qwen2.5:7b-instruct
- model name for API endpoint: qwen2.5:7b-instruct

4) Chat App erstellen (ohne RAG)
- In Dify: Create App -> Chat App.
- Provider: OpenAI-API-compatible wÃ¤hlen, dein Modell auswÃ¤hlen.
- Knowledge/RAG: nicht hinzufÃ¼gen bzw. deaktiviert lassen.
- Opening message: z. B. Willkommen! Ich begleite dich Schritt fÃ¼r Schritt durch Design Thinking. Nenne mir zuerst kurz dein Problem oder Ziel. Danach starten wir mit Phase 1 â€“ Empathize.
- System Prompt (einfÃ¼gen und bei Bedarf anpassen):
  >   - Rolle und Ziel:
  >   - Du bist ein methodischer Moderator, der den Anwender strikt durch ein mehrstufiges Design-Thinking-Verfahren fÃ¼hrt, um eine LÃ¶sung fÃ¼r ein von ihm benanntes Problem zu entwickeln. Keine Wissensanreicherung aus externen Quellen verwenden.
  > - Vorgehen in 5 Phasen (strikt nacheinander):
  >   - Phase 1 â€“ Empathize: KlÃ¤re Zielgruppe, Kontext, BedÃ¼rfnisse, Schmerzpunkte. Sammle Kernbeobachtungen in Stichpunkten. Frage nach, bis die Infos ausreichend sind. Abschlussoutput: â€œEmpathize-Zusammenfassungâ€.
  >   - Phase 2 â€“ Define: Verdichte zu 1â€“2 klaren Problemstatements mit â€œWer? Was? Warum wichtig?â€. Lass den Anwender bestÃ¤tigen/Ã¤ndern. Abschlussoutput: â€œProblemdefinition v1â€.
  >   - Phase 3 â€“ Ideate: Erzeuge 5â€“8 LÃ¶sungsansÃ¤tze mit kurzen Vor-/Nachteilen. Bitte den Anwender, 1â€“2 Favoriten auszuwÃ¤hlen. Abschlussoutput: â€œTop-Ideenâ€.
  >   - Phase 4 â€“ Prototype: Beschreibe fÃ¼r die Top-Idee(n) einen Low-Fidelity-Prototypen (Ziele, Hauptfunktionen, grober Userflow/Screen-Skizze in Text). Abschlussoutput: â€œPrototyp-Plan v1â€.
  >   - Phase 5 â€“ Test: Formuliere 5â€“8 Testfragen/Tasks, definiere Erfolgskriterien und nÃ¤chste Schritte.
  > - Interaktionsregeln:
  >   - Stelle immer nur eine Phase zugleich dar und frage explizit â€œWeiter mit Phase X?â€.
  >   - Nutze klare Listen, kurze SÃ¤tze, deutschsprachig, inklusiver Ton.
  >   - Referenziere die Nutzerantworten korrekt und iteriere.
  > - Abschluss:
  >   - Erzeuge eine Endzusammenfassung mit: Problemstatement, Annahmen, ausgewÃ¤hlte Idee(n), Prototyp-Plan, Testplan, Risiken/NÃ¤chste Schritte.
- Publish und testen.

5) Template exportieren (fÃ¼r die Weitergabe)
- In der App: Export Template -> Datei sichern (z. B. dify-template.json).
- README und Template ins Repo/Ordner legen â€“ fertig. ðŸŒŸ


## FAQ & Troubleshooting ðŸ§©

- Warum funktioniert die URL http://host.docker.internal:11434/v1? ðŸ”
  - Dify lÃ¤uft in Dockerâ€‘Containern. Damit ein Container (Dify) den Ollamaâ€‘Dienst auf deinem Hostâ€‘Rechner erreicht, gibt es host.docker.internal (auf macOS/Windows). Das ist eine spezielle Hostnameâ€‘BrÃ¼cke vom Container zum Host.
  - Der /v1â€‘Pfad: Ollama bietet eine OpenAI-kompatible API unter dem Pfad /v1. Dify erwartet genau dieses Schema. Deshalb muss die Baseâ€‘URL auf /v1 enden.
  - Linuxâ€‘Sonderfall: host.docker.internal ist dort oft nicht vordefiniert. Nimm stattdessen die Gatewayâ€‘IP der Dockerâ€‘Bridge (hÃ¤ufig 172.17.0.1). Du findest sie mit: docker network inspect bridge und suchst nach "Gateway".

- Es gibt keinen â€œChatâ€â€‘Typ bei â€œModel Typeâ€.
  - Korrekt. In Dify heiÃŸt das einfach â€œLLMâ€. Das ist der Chat/Textâ€‘Generierungstyp.

- 404 Not Found bei APIâ€‘Aufruf.
  - Meist fehlt /v1 am Ende der Baseâ€‘URL. Nutze z. B. http://host.docker.internal:11434/v1

- Verbindung fehlgeschlagen von Dify zu Ollama.
  - macOS/Windows: Nutze host.docker.internal, nicht localhost.
  - Linux: Nutze http://172.17.0.1:11434/v1 oder deinen Bridgeâ€‘Gatewayâ€‘Host (docker network inspect bridge).
  - PrÃ¼fe, ob Ollama lÃ¤uft: ollama list

- â€œModel not foundâ€ oder â€œThe model does not existâ€.
  - Der Eintrag â€œmodel name for API endpointâ€ muss exakt dem Ollamaâ€‘Tag entsprechen (z. B. llama3.1:8b). PrÃ¼fe mit ollama list oder curl http://localhost:11434/api/tags

- Was trage ich bei â€œAPI Keyâ€ ein?
  - Irgendeinen nichtâ€‘leeren String (z. B. ollama). Ollama prÃ¼ft das nicht, Dify will aber ein Feld.

- Ist .yml und .yaml dasselbe?
  - Ja. Beides ist YAML. Docker Compose akzeptiert beide. Falls der Dateiname abweicht, nutze docker compose -f datei.yaml up -d

- Responses sind langsam.
  - Kleinere Modelle nutzen (z. B. qwen2.5:7b-instruct).
  - In Dify die Maxâ€‘Token reduzieren.
  - Genug RAM/CPU sicherstellen.

- Kann ich ein anderes Modell nutzen?
  - Ja. In Ollama den passenden Tag ziehen (ollama pull ...), danach in Dify dasselbe Vorgehen: Model Name und model name for API endpoint identisch zum Tag.

- â€œlocalhostâ€ funktioniert nicht aus Dify.
  - Dify lÃ¤uft im Container. localhost meint dann den Container selbst, nicht deinen Host. Daher host.docker.internal (Mac/Win) bzw. Bridgeâ€‘Gateway (Linux).

- Linux: Wie finde ich die Gatewayâ€‘IP?
  - docker network inspect bridge und nach "Gateway" suchen. Typisch ist 172.17.0.1

- Portâ€‘Konflikte beim Start von Dify.
  - PrÃ¼fe, ob 3000 (Dify UI) oder weitere in der Compose belegte Ports frei sind. Passe Ports in der docker-compose.yaml an, falls nÃ¶tig, und starte neu.

- Ich habe versehentlich Knowledge/RAG aktiviert.
  - In der App die Knowledgeâ€‘Sektion entfernen bzw. sicherstellen, dass kein Datenspeicher verbunden ist. FÃ¼r Agent 1 brauchen wir kein RAG.

- Wie exportiere/importiere ich das Template?
  - In der App: Export Template -> Datei sichern. Import analog Ã¼ber Import Template.
