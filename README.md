## Installation und Setup der Software und integrieren der Sprachmodelle:
Willkommen zur KI-Lanparty Laborumgebung. Dieses Repository f√ºhrt Sie in die Einrichtung einer sofort einsatzbereiten Demo-Umgebung f√ºr eine KI-Lanparty mit drei Prototypen: Zwei KI-Agenten basierend auf einem Low-Code-Framework und einen On-Edge-Sprachassistenten. Die Software l√§uft lokal auf den Teilnehmerrechnern bzw. bereitgestellter Hardware. Das Setup liefert klare Templates und eine Schritt-f√ºr-Schritt-Anleitung, damit Teilnehmer eigene Experimente durchf√ºhren und Anpassungen einfach vornehmen k√∂nnen. Die Bereitstellung erfolgt √ºber GitHub, damit Modelle, Vorlagen und Dokumentationen zentral verf√ºgbar sind.

Kurzbeschreibung der verwendeten Software

Dify: Low-Code-Framework zur Erstellung KI-gesteuerter Prototypen; liefert Vorlagen, Systemprompts und einfache Integrationen f√ºr KI-Agenten.
Open WebUI: Grafische Oberfl√§che zur Interaktion mit lokal laufenden Modellen; erm√∂glicht einfachen Modellwechsel und -anpassungen.
Ollama: Lokaler Runner zur Bereitstellung von Modellen (z. B. llama3:8b, qwen3:8b); erm√∂glicht die direkte Ausf√ºhrung von KI-Modellen ohne Cloud-Anbindung.

## Schritt f√ºr Schritt üçÄ

1) Ollama installieren und Modell vorbereiten
- macOS: `brew install ollama`
- Windows: Installer von ollama.com
- Linux: curl-Install gem√§√ü ollama.com
- Service starten (l√§uft meist automatisch nach Installation).
- Modell ziehen:
  - `ollama pull llama3.1:8b`
  - Alternativ: `ollama pull qwen2.5:7b-instruct`
- Testen:
  - `ollama list` (zeigt installierte Modelle)
  - Optional: einfacher API‚ÄëTest
    - `curl http://localhost:11434/v1/chat/completions -H "Content-Type: application/json" -H "Authorization: Bearer ollama" -d '{"model":"llama3.1:8b","messages":[{"role":"user","content":"Hallo"}]}'`

2) Dify lokal starten
- Repository klonen:
  - `git clone https://github.com/langgenius/dify`
- In den Ordner wechseln, der die docker-compose.yaml enth√§lt (siehe Dify‚ÄëREADME).
- .env anlegen:
  - Kopiere .env.example nach .env
- Start:
  - `docker compose up -d`
- UI √∂ffnen:
  - http://localhost:3000
- Admin‚ÄëKonto anlegen.

3) Dify mit Ollama verbinden (OpenAI-API-kompatibel) üîå
Im Dify‚ÄëUI: 

- Settings:

<img width="911" height="1096" alt="image" src="https://github.com/user-attachments/assets/520be460-85f1-4ed1-aedc-40ec4ae99a15" />

- Modellanbieter -> Nach "OpenAI-API-compatible" suchen -> Modell hinzuf√ºgen

<img width="1129" height="885" alt="image" src="https://github.com/user-attachments/assets/7d57814e-bad2-4b93-8587-86903e9ffa37" />


Felder ausf√ºllen (Beispiel: llama3.1:8b):
- Model Type: LLM
- Model Name: llama3.1:8b
- Authorization Name: Authorization
- Model display name: Llama 3.1 8B (Ollama)  [frei w√§hlbar]
- API Key: ollama  [beliebiger String, wird von Ollama nicht gepr√ºft]
- API endpoint URL:
  - macOS/Windows: http://host.docker.internal:11434/v1
  - Linux: http://172.17.0.1:11434/v1 (oder das Gateway aus docker network inspect bridge)
- model name for API endpoint: llama3.1:8b  [muss exakt zum Ollama‚ÄëTag passen]

<img width="773" height="1293" alt="image" src="https://github.com/user-attachments/assets/81869b51-e41e-4480-8793-747bfae8fbd3" />


Alternative Modellwerte (wenn du Qwen nutzt):
- Model Name: qwen2.5:7b-instruct
- model name for API endpoint: qwen2.5:7b-instruct

4) Chat App erstellen (ohne RAG)
- In Dify: Create App -> Chat App.
- Provider: OpenAI-API-compatible w√§hlen, dein Modell ausw√§hlen.
- Knowledge/RAG: nicht hinzuf√ºgen bzw. deaktiviert lassen.
- Opening message: z. B. Willkommen! Ich begleite dich Schritt f√ºr Schritt durch Design Thinking. Nenne mir zuerst kurz dein Problem oder Ziel. Danach starten wir mit Phase 1 ‚Äì Empathize.
- System Prompt (einf√ºgen und bei Bedarf anpassen):
  >   - Rolle und Ziel:
  >   - Du bist ein methodischer Moderator, der den Anwender strikt durch ein mehrstufiges Design-Thinking-Verfahren f√ºhrt, um eine L√∂sung f√ºr ein von ihm benanntes Problem zu entwickeln. Keine Wissensanreicherung aus externen Quellen verwenden.
  > - Vorgehen in 5 Phasen (strikt nacheinander):
  >   - Phase 1 ‚Äì Empathize: Kl√§re Zielgruppe, Kontext, Bed√ºrfnisse, Schmerzpunkte. Sammle Kernbeobachtungen in Stichpunkten. Frage nach, bis die Infos ausreichend sind. Abschlussoutput: ‚ÄúEmpathize-Zusammenfassung‚Äù.
  >   - Phase 2 ‚Äì Define: Verdichte zu 1‚Äì2 klaren Problemstatements mit ‚ÄúWer? Was? Warum wichtig?‚Äù. Lass den Anwender best√§tigen/√§ndern. Abschlussoutput: ‚ÄúProblemdefinition v1‚Äù.
  >   - Phase 3 ‚Äì Ideate: Erzeuge 5‚Äì8 L√∂sungsans√§tze mit kurzen Vor-/Nachteilen. Bitte den Anwender, 1‚Äì2 Favoriten auszuw√§hlen. Abschlussoutput: ‚ÄúTop-Ideen‚Äù.
  >   - Phase 4 ‚Äì Prototype: Beschreibe f√ºr die Top-Idee(n) einen Low-Fidelity-Prototypen (Ziele, Hauptfunktionen, grober Userflow/Screen-Skizze in Text). Abschlussoutput: ‚ÄúPrototyp-Plan v1‚Äù.
  >   - Phase 5 ‚Äì Test: Formuliere 5‚Äì8 Testfragen/Tasks, definiere Erfolgskriterien und n√§chste Schritte.
  > - Interaktionsregeln:
  >   - Stelle immer nur eine Phase zugleich dar und frage explizit ‚ÄúWeiter mit Phase X?‚Äù.
  >   - Nutze klare Listen, kurze S√§tze, deutschsprachig, inklusiver Ton.
  >   - Referenziere die Nutzerantworten korrekt und iteriere.
  > - Abschluss:
  >   - Erzeuge eine Endzusammenfassung mit: Problemstatement, Annahmen, ausgew√§hlte Idee(n), Prototyp-Plan, Testplan, Risiken/N√§chste Schritte.
- Publish und testen.

5) Template exportieren (f√ºr die Weitergabe)
- In der App: Export Template -> Datei sichern (z. B. dify-template.json).
- README und Template ins Repo/Ordner legen ‚Äì fertig. üåü


## FAQ & Troubleshooting üß©

- Warum funktioniert die URL http://host.docker.internal:11434/v1? üîç
  - Dify l√§uft in Docker‚ÄëContainern. Damit ein Container (Dify) den Ollama‚ÄëDienst auf deinem Host‚ÄëRechner erreicht, gibt es host.docker.internal (auf macOS/Windows). Das ist eine spezielle Hostname‚ÄëBr√ºcke vom Container zum Host.
  - Der /v1‚ÄëPfad: Ollama bietet eine OpenAI-kompatible API unter dem Pfad /v1. Dify erwartet genau dieses Schema. Deshalb muss die Base‚ÄëURL auf /v1 enden.
  - Linux‚ÄëSonderfall: host.docker.internal ist dort oft nicht vordefiniert. Nimm stattdessen die Gateway‚ÄëIP der Docker‚ÄëBridge (h√§ufig 172.17.0.1). Du findest sie mit: docker network inspect bridge und suchst nach "Gateway".

- Es gibt keinen ‚ÄúChat‚Äù‚ÄëTyp bei ‚ÄúModel Type‚Äù.
  - Korrekt. In Dify hei√üt das einfach ‚ÄúLLM‚Äù. Das ist der Chat/Text‚ÄëGenerierungstyp.

- 404 Not Found bei API‚ÄëAufruf.
  - Meist fehlt /v1 am Ende der Base‚ÄëURL. Nutze z. B. http://host.docker.internal:11434/v1

- Verbindung fehlgeschlagen von Dify zu Ollama.
  - macOS/Windows: Nutze host.docker.internal, nicht localhost.
  - Linux: Nutze http://172.17.0.1:11434/v1 oder deinen Bridge‚ÄëGateway‚ÄëHost (docker network inspect bridge).
  - Pr√ºfe, ob Ollama l√§uft: ollama list

- ‚ÄúModel not found‚Äù oder ‚ÄúThe model does not exist‚Äù.
  - Der Eintrag ‚Äúmodel name for API endpoint‚Äù muss exakt dem Ollama‚ÄëTag entsprechen (z. B. llama3.1:8b). Pr√ºfe mit ollama list oder curl http://localhost:11434/api/tags

- Was trage ich bei ‚ÄúAPI Key‚Äù ein?
  - Irgendeinen nicht‚Äëleeren String (z. B. ollama). Ollama pr√ºft das nicht, Dify will aber ein Feld.

- Ist .yml und .yaml dasselbe?
  - Ja. Beides ist YAML. Docker Compose akzeptiert beide. Falls der Dateiname abweicht, nutze docker compose -f datei.yaml up -d

- Responses sind langsam.
  - Kleinere Modelle nutzen (z. B. qwen2.5:7b-instruct).
  - In Dify die Max‚ÄëToken reduzieren.
  - Genug RAM/CPU sicherstellen.

- Kann ich ein anderes Modell nutzen?
  - Ja. In Ollama den passenden Tag ziehen (ollama pull ...), danach in Dify dasselbe Vorgehen: Model Name und model name for API endpoint identisch zum Tag.

- ‚Äúlocalhost‚Äù funktioniert nicht aus Dify.
  - Dify l√§uft im Container. localhost meint dann den Container selbst, nicht deinen Host. Daher host.docker.internal (Mac/Win) bzw. Bridge‚ÄëGateway (Linux).

- Linux: Wie finde ich die Gateway‚ÄëIP?
  - docker network inspect bridge und nach "Gateway" suchen. Typisch ist 172.17.0.1

- Port‚ÄëKonflikte beim Start von Dify.
  - Pr√ºfe, ob 3000 (Dify UI) oder weitere in der Compose belegte Ports frei sind. Passe Ports in der docker-compose.yaml an, falls n√∂tig, und starte neu.

- Ich habe versehentlich Knowledge/RAG aktiviert.
  - In der App die Knowledge‚ÄëSektion entfernen bzw. sicherstellen, dass kein Datenspeicher verbunden ist. F√ºr Agent 1 brauchen wir kein RAG.

- Wie exportiere/importiere ich das Template?
  - In der App: Export Template -> Datei sichern. Import analog √ºber Import Template.
