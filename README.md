# KI-LAN-Party Demo-Umgebung

Willkommen zur KI-LAN-Party Arbeitsumgebung! üëã

Dieses Repository f√ºhrt Sie in die Einrichtung einer sofort einsatzbereiten Demo-Umgebung f√ºr eine KI-LAN-Party mit drei Prototypen: Zwei KI-Agenten basierend auf einem Low-Code-Framework und einen Sprachassistenten.

Die Software l√§uft lokal auf den Teilnehmerrechnern bzw. bereitgestellter Hardware. Das Setup liefert klare Templates und Anleitungen, damit Teilnehmer selbst Experimente durchf√ºhren k√∂nnen.

## √úbersicht der Agenten

- **Agent 1**: Design Thinking Assistant (ohne RAG) ‚Üí [Setup-Anleitung](./Agent_1/README.md)
- **Agent 2**: Dokumentations-Experte (mit RAG) ‚Üí [Setup-Anleitung](./Agent_2/README.md)
- **Agent 3**: Interview-Assistant mit Sprachsteuerung ‚Üí [Setup-Anleitung](./Agent_3/README.md)

## Verwendete Software

- **[Docker](https://docs.docker.com/)**: Container-Plattform, die Anwendungen inklusive ihrer Abh√§ngigkeiten in isolierten Containern verpackt
- **[Dify](https://docs.dify.ai/en/introduction)**: Low-Code-Framework zur Erstellung KI-gesteuerter Prototypen
- **[Open WebUI](https://docs.openwebui.com/)**: Grafische Oberfl√§che zur Interaktion mit lokal laufenden Modellen
- **[Ollama](https://docs.ollama.com/)**: Lokaler Runner zur Bereitstellung von Modellen (z.B. `llama3.1:8b`, `qwen3:8b`)

## Grundlegendes Setup

Das Setup erfolgt in drei wesentlichen Schritten:

1. **Ollama** - Ein lokaler KI-Model Runner (f√ºr Agents 1-3)
2. **Docker Desktop** - Die Container-Platform (wird f√ºr Dify und Open WebUI)
3. **Dify** - Ein Low-Code AI-Framework (f√ºr Agents 1-2)

### Ollama installieren und Modelle vorbereiten

**Installation je nach Betriebssystem:**
- **macOS**: Ollama-Installer von [ollama.com](https://ollama.com) oder via Homebrew: `brew install ollama`
- **Windows**: Ollama-Installer von [ollama.com](https://ollama.com)
- **Linux**: curl-Installation gem√§√ü [ollama.com](https://ollama.com)

**Sprachmodelle herunterladen:**

Option 1 - Terminal:
```bash
ollama pull llama3.1:8b
ollama pull qwen3:8b
# Testen:
ollama list
```

Option 2 - Ollama UI:
1. Starte Ollama
2. Suche die gew√ºnschten Modelle (`llama3.1:8b` und `qwen3:8b`)
  
   ![Ollama Modell suchen](https://github.com/user-attachments/assets/a4fcb0c1-0ffa-49a9-845c-a9024a7b1e02)

3. Sende eine Nachricht im Chat, um den Download zu starten

   ![Ollama Download starten](https://github.com/user-attachments/assets/153270f0-f50a-4bdb-81db-0fca22239f6a)

4. Lade beide Modelle herunter (`llama3.1:8b` und `qwen3:8b`)

### Docker Desktop installieren

1. Lade Docker Desktop von [docker.com](https://docker.com) herunter
2. Installiere und starte Docker Desktop

> **macOS-Hinweis**: Bei MacBooks kann eine Fehlermeldung bez√ºglich Rosetta erscheinen. Klicke auf das Fenster ‚Üí Installieren ‚Üí bei Fehlermeldungen auf 'Retry' klicken. Wenn sich Docker schlie√üt, wurde Rosetta erfolgreich installiert. Docker anschlie√üend manuell neu starten.
  
### Dify lokal starten

1. Dify wird als Repository bereitgestellt. Nutze das Terminal, um das Repository zu klonen und wechsle danach in das Docker-Verzeichnis:
   ```bash
   git clone https://github.com/langgenius/dify
   cd dify/docker
   ```

2. dify ben√∂tigt eine Umgebungsdatei `.env`, in der die Konfigurationen vorgenommen wird. In unserem Fall k√∂nnen wir die Beispiel-Datei kopieren:
   - Kopiere `.env.example` zu `.env`: 
   ```bash
   cp .env.example .env
   ```
   - Falls du die Datei per Finder/Explorer kopieren m√∂chtest, musst du gegebenenfalls die **versteckten Dateien anzeigen**:
     - **macOS**: `Cmd + Shift + .`
     - **Windows**: Datei-Explorer ‚Üí Ansicht ‚Üí Versteckte Elemente anzeigen

3. Dify starten:
   ```bash
   docker compose up -d
   ```

4. UI √∂ffnen: [http://localhost:3000](http://localhost:3000)

5. Beim ersten Start muss ein lokales Admin-Konto angelegt werden. Dieses wird nur lokal gespeichert.

> **Container-Verwaltung**: In Docker Desktop unter **Containers** den `docker`-Container stoppen/starten √ºber die **Actions**-Buttons. Alternative kann der Befehl `docker compose down` genutzt werden, um alle Container zu stoppen. Im Terminal kann mit `docker compose ps` der Status gepr√ºft werden, und mit `docker compose logs -f` die Logs live verfolgt werden.

### Dify mit Ollama verbinden

1. In Dify UI: **Settings** aufrufen

   ![Dify Settings](https://github.com/user-attachments/assets/520be460-85f1-4ed1-aedc-40ec4ae99a15)

2. **Model Providers** ‚Üí Suche nach "OpenAI-API-compatible" ‚Üí **Add model**

   ![OpenAI Compatible Provider](https://github.com/user-attachments/assets/7d57814e-bad2-4b93-8587-86903e9ffa37)

3. Felder ausf√ºllen (Beispiel f√ºr `llama3.1:8b`):
   - **Model Type**: `LLM`
   - **Model Name**: `llama3.1:8b`
   - **Authorization Name**: `Authorization`
   - **Model display name**: `Llama 3.1 8B (Ollama)` (frei w√§hlbar)
   - **API Key**: `ollama` (beliebiger String, wird nicht gepr√ºft)
   - **API endpoint URL**:
     - **macOS/Windows**: `http://host.docker.internal:11434/v1`
     - **Linux**: `http://172.17.0.1:11434/v1`
   - **Model name for API endpoint**: `llama3.1:8b` (muss exakt zum Ollama-Tag passen)

   ![Modell Konfiguration](https://github.com/user-attachments/assets/81869b51-e41e-4480-8793-747bfae8fbd3)

**Windows/WSL-Troubleshooting**: Bei Verbindungsproblemen zwischen Dify und Ollama:
- Umgebungsvariable setzen: `OLLAMA_HOST=0.0.0.0:11434`
- WSL-IP finden mit `ipconfig` in PowerShell
- URL verwenden: `http://<WSL-IP>:11434/v1`
- Ollama neu starten

---

## ‚úÖ Setup abgeschlossen!

**Schnellstart**: Gehe direkt zu den Agent-Anleitungen:
- [Agent 1: Design Thinking Assistant](./Agent_1/README.md)
- [Agent 2: Dokumentations-Experte](./Agent_2/README.md)
- [Agent 3: Interview-Assistant](./Agent_3/README.md)

---


## N√ºtzliche Tipps & Tricks

### Grundlagen der Agenten-Nutzung

**Chat-Format**: Jeder neue Chat ist eine neue Unterhaltung. Du kannst Memory-Funktionen aktivieren, damit sich Agenten an vorherige Chats erinnern.

**Mehrere Agenten**: Erstelle verschiedene Agenten f√ºr verschiedene Anwendungsf√§lle √ºber die Dify-Hauptseite.

### Prompt-Engineering

**Grundprinzip**: Je detaillierter der Kontext, desto besser die Antworten. Stelle dir vor, du erkl√§rst alles einer entfernten Person, die Expertin in vielen Bereichen ist.

**Variablen verwenden**:
- Im Prompt: `{{variablenname}}`
- Beispiel: `Nenne mich bei meinem Namen {{name}}`

**Template-Frameworks**: Nutze etablierte Frameworks wie AUTOMAT f√ºr strukturierte Prompts.

### Dokumente hochladen (RAG)

1. **Knowledge** ‚Üí **Create Knowledge**

   ![Knowledge Tab](https://github.com/user-attachments/assets/ca4d8b25-b9fe-431a-8604-dc58b6d084b8)

2. **Import aus Textdatei** ‚Üí Drag & Drop ‚Üí **High Quality** ‚Üí **Save & Process**

   ![Dokument Upload](https://github.com/user-attachments/assets/de9406e6-5bd1-4fe8-b46b-cf6c1921e591)

3. In der App: **Context** ‚Üí Dokumente hinzuf√ºgen

   ![Context hinzuf√ºgen](https://github.com/user-attachments/assets/8b1b30a8-4068-4c05-a297-b9797178ae48)

### Wichtige Einschr√§nkungen

- **Kontextverlust**: Bei langen Chats oder gro√üen Dokumenten kann der Agent fr√ºhere Informationen vergessen
- **Halluzinationen**: KI-Agenten k√∂nnen falsche Informationen generieren - immer Antworten √ºberpr√ºfen


## FAQ & Troubleshooting

### Verbindungsprobleme

**Warum `host.docker.internal:11434/v1`?**
- Dify l√§uft im Docker-Container und braucht eine Br√ºcke zum Host-System
- `/v1` ist f√ºr OpenAI-kompatible API erforderlich
- **Linux**: Nutze statt dem Host die Gateway-IP: `docker network inspect bridge`
- **Windows/WSL**: Nutze statt dem Host die WSL-IP, die Du mit `ipconfig` ermitteln kannst.

**Verbindung fehlgeschlagen:**
```bash
# Ollama-Status pr√ºfen
ollama list
# API testen
curl http://localhost:11434/api/tags
```

**"Model not found":**
- Model Name muss exakt dem Ollama-Tag entsprechen
- Pr√ºfe verf√ºgbare Modelle: `ollama list`

### H√§ufige Fragen

**API Key bei Ollama?** Beliebiger String (z.B. `ollama`) - wird nicht gepr√ºft

**Chat vs. LLM Model Type?** In Dify hei√üt es "LLM" (nicht "Chat")

**Langsame Antworten?**
- Kleinere Modelle verwenden
- Max-Token in Dify reduzieren
- RAM/CPU-Ressourcen pr√ºfen

**Template Import/Export:**
- **Export**: App ‚Üí Export Template
- **Import**: Create App ‚Üí Import DSL File

### Port-Konflikte

Standardports pr√ºfen:
- Dify UI: Port `3000`
- Ollama: Port `11434`

Bei Konflikten: Ports in `docker-compose.yaml` anpassen
