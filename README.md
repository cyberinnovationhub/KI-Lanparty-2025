# README (Allgemein) - KI-LAN-Party
Willkommen zur KI-LAN-Party Laborumgebung! ðŸ‘‹ 

Dieses Repository fÃ¼hrt Sie in die Einrichtung einer sofort einsatzbereiten Demo-Umgebung fÃ¼r eine KI-LAN-Party mit drei Prototypen: Zwei KI-Agenten basierend auf einem Low-Code-Framework und einen On-Edge-Sprachassistenten. 

Die Software lÃ¤uft lokal auf den Teilnehmerrechnern bzw. bereitgestellter Hardware. Das Setup liefert klare Templates und Anleitungen, damit Teilnehmer selbst Experimente durchfÃ¼hren kÃ¶nnen.

### Kurzbeschreibung der verwendeten Software

- Docker: Docker ist eine Container-Plattform, die Anwendungen inklusive ihrer AbhÃ¤ngigkeiten in isolierten Containern verpackt.

- Dify: Low-Code-Framework zur Erstellung KI-gesteuerter Prototypen; liefert Vorlagen, Systemprompts und einfache Integrationen fÃ¼r KI-Agenten.
  
- Open WebUI: Grafische OberflÃ¤che zur Interaktion mit lokal laufenden Modellen; ermÃ¶glicht einfachen Modellwechsel und -anpassungen.

- Ollama: Lokaler Runner zur Bereitstellung von Modellen (z. B. llama3:8b, qwen3:8b); ermÃ¶glicht die direkte AusfÃ¼hrung von KI-Modellen ohne Cloud-Anbindung.

## Setup

### 1) Ollama installieren und Modell vorbereiten
- macOS: Installer von ollama.com / Homebrew installieren, dann -> `brew install ollama`
- Windows: Installer von ollama.com
- Linux: curl-Install gemÃ¤ÃŸ ollama.com

#### 1.1) Sprachmodelle Ã¼ber Terminal ziehen
  - (via Ollama App UI: folge Step 1.2)
  - via Terminal Befehl: `ollama pull llama3.1:8b` und danach `ollama pull qwen3:8b`
    - Testen: `ollama list` (zeigt installierte Modelle)

#### 1.2) Sprachmodelle Ã¼ber Ollama UI ziehen

- Starte Ollama
- Suche die gewÃ¼nschten Modelle (`llama3:8b` und `qwen3:8b`):
  
  <img width="470" height="351" alt="image" src="https://github.com/user-attachments/assets/a4fcb0c1-0ffa-49a9-845c-a9024a7b1e02" />

- Sende eine Nachricht in den Chat, um den Download zu starten

  <img width="454" height="336" alt="image" src="https://github.com/user-attachments/assets/153270f0-f50a-4bdb-81db-0fca22239f6a" />

- Lade beide Modelle runter (`llama3.1:8b` und `qwen3:8b`)
  
### 3) Dify lokal starten
- Repository klonen:
  - `git clone https://github.com/langgenius/dify`
- In den Ordner wechseln, der die docker-compose.yaml enthÃ¤lt (fÃ¼r mehr Infos, siehe Difyâ€‘README).
- neue .env Datei anlegen:
  - Kopiere .env.example Inhalt nach .env
- Start:
  - `docker compose up -d`
- UI Ã¶ffnen:
  - http://localhost:3000 oder `localhost:3000` als URL eingeben und Enter drÃ¼cken
- Adminâ€‘Konto anlegen.
---

> ## ðŸš¦ WÃ¤hle: Templates vs. Manuelles Setup
>
> **Du willst den schnellsten und einfachsten Einstieg?**  
> ðŸ‘‰ **Springe direkt zur [Agent 1 README](./Agent_1/README.md) und nutze die fertigen Templates.**
>
> **Du mÃ¶chtest alles selbst lernen und individuell anpassen?**  
> ðŸ‘‡ **Lies unten weiter fÃ¼r eine Schritt-fÃ¼r-Schritt-Anleitung zum manuellen Setup.**  
> *(Hinweis: Das ist fortgeschrittener und dauert lÃ¤nger, gibt dir aber volle Kontrolle.)*

---

### 4) Dify mit Ollama verbinden (OpenAI-API-kompatibel) ðŸ”Œ
Im Difyâ€‘UI: 

- Settings:

<img width="911" height="1096" alt="image" src="https://github.com/user-attachments/assets/520be460-85f1-4ed1-aedc-40ec4ae99a15" />

- Modellanbieter -> Nach "OpenAI-API-compatible" suchen -> Modell hinzufÃ¼gen

<img width="1129" height="885" alt="image" src="https://github.com/user-attachments/assets/7d57814e-bad2-4b93-8587-86903e9ffa37" />


Felder ausfÃ¼llen (Beispiel: llama3.1:8b):
- Model Type: LLM
- Model Name: `llama3.1:8b`
- Authorization Name: `Authorization`
- Model display name: `Llama 3.1 8B (Ollama)`  [frei wÃ¤hlbar]
- API Key: `ollama`  [beliebiger String, wird von Ollama nicht geprÃ¼ft]
- API endpoint URL:
  - macOS/Windows: http://host.docker.internal:11434/v1
  - Linux: http://172.17.0.1:11434/v1 (oder das Gateway aus docker network inspect bridge)
- model name for API endpoint: `llama3.1:8b`  [muss exakt zum Ollamaâ€‘Tag passen]

<img width="773" height="1293" alt="image" src="https://github.com/user-attachments/assets/81869b51-e41e-4480-8793-747bfae8fbd3" />

### HÃ¤ufige Probleme:
- Manchmal kann es bei Windows im Zusammenspiel von WSL, docker und dify zu Verbindungsproblemen kommen. In diesem Fall hilft:
  - Setze die Umgebungsvariable OLLAMA_HOST auf `0.0.0.0:11434`
  - Verwende die URL `http://<WSL-IP>:11434/v1` in Dify
  - Die WSL-IP findest du mit `ipconfig` in der PowerShell heraus
  - Danach starte Ollama neu


> ### Good job! You finished the initial setup.
> ### You can move on to the Agent_1 README now. 
> Or, if you're still a bit unsure, you can read the Usage / FAQ Sections below. ðŸ‘‡


## Usage and Configuration

### Chat Format
- The AI Agent works as a Chat format, meaning every new chat is like a new conversation. You can activate the 'memory' features in the settings so that the Agent remembers things from other chats and conversations, but this uses more resources - and sometimes you might just want to start fresh every time. 

### Multiple Agents
- Dify gives you the opportunity to create multiple Agents - meaning that, if you have different use cases, you can create a dedicated Agent for each use case. You can create Agents from the main Dify page (click on the Dify logo).

### How to write prompts (Prompt engineering)
- You can change the prompt however you like. The current prompt is just an example. Your AI Agent could be a lawyer, teacher, strategist, or anything else. Just keep in mind that, the more detailed context you give it, the better and more accurate it will be. Imagine this is a person living far away that doesn't know anything about you or what you're trying to do. And you're chatting online with this person (who is an expert on many things), so you need to give them a detailed background of the problem.

### Placeholder template prompt
- If you need a placeholder template as a starter prompt, you can use the AUTOMAT framework, but there are many you can find online. Here you will find some basic ones: https://medium.com/the-generator/the-perfect-prompt-prompt-engineering-cheat-sheet-d0b9c62a2bba

### Variables
- You can add variables for the prompt. Inside the prompt text, just add two curly brackets with a variable name inbetween (`{{name}}`) and then add it as a variable in the Dify menu below the prompt text. Depending on how you included the variable inside your prompt, the Agent will use that variable accordingly. A simple example you can test is telling it to call you by your name. (`Call me by my name {{name}}`).

### Uploading documents
- It is possible to upload documents that the AI Agent will later use as an information source. This would be useful, for example, if you had a large PDF file with lots of information about a certain topic. You could upload it and ask the AI specific questions about the topic. Or summarize the whole document. Or improve the spelling and grammar. There are many use cases.
- To do this, you have to go to the Main Menu Knowledge Tab (top bar) > Create Knowledge > Import from file > Select/Drag&Drop your file > Next > High Quality > Save & Process. After you upload it there, you can go to your chat in studio view, Knowledge Dropdown on the left side (not top bar), and just select the document you uploaded earlier. Now, the AI Agent will know everything inside that document.

### Other features
- There are many other possible features that take a bit more time to set up (e.g. Vision feature to analyse uploaded Images). If you have some experience and have the extra time, you can try setting them up. 

### Limitations
- There are limitations when it comes to the amount of information the AI Agent can process. If you chat to the Agent for long enough, it will start to forget what you talked about in the beginning, and might get confused halfway through. Also, if you upload documents for the AI to use as a source of information, and the documents are too large, the AI Agent might also forget things or lose context. You can learn the exact limitations via trial and error.
- The AI Agents are not perfect and they often hallucinate. So you definitely need to double check the information you get from them, since they will confidently give you wrong answers.


## FAQ & Troubleshooting ðŸ§©

- Warum funktioniert die URL http://host.docker.internal:11434/v1? ðŸ”
  - Dify lÃ¤uft in Dockerâ€‘Containern. Damit ein Container (Dify) den Ollamaâ€‘Dienst auf deinem Hostâ€‘Rechner erreicht, gibt es host.docker.internal (auf macOS/Windows). Das ist eine spezielle Hostnameâ€‘BrÃ¼cke vom Container zum Host.
  - Der /v1â€‘Pfad: Ollama bietet eine OpenAI-kompatible API unter dem Pfad /v1. Dify erwartet genau dieses Schema. Deshalb muss die Baseâ€‘URL auf /v1 enden.
  - Linuxâ€‘Sonderfall: host.docker.internal ist dort oft nicht vordefiniert. Nimm stattdessen die Gatewayâ€‘IP der Dockerâ€‘Bridge (hÃ¤ufig 172.17.0.1). Du findest sie mit: docker network inspect bridge und suchst nach "Gateway".

- Es gibt keinen â€œChatâ€â€‘Typ bei â€œModel Typeâ€.
  - Korrekt. In Dify heiÃŸt das einfach â€œLLMâ€. Das ist der Chat/Textâ€‘Generierungstyp.

- 404 Not Found bei APIâ€‘Aufruf.
  - Meist fehlt /v1 am Ende der Baseâ€‘URL. Nutze z. B. http://host.docker.internal:11434/v1

- Verbindung fehlgeschlagen von Dify zu Ollama.
  - macOS/Windows: Nutze host.docker.internal, nicht localhost.
  - Linux: Nutze http://172.17.0.1:11434/v1 oder deinen Bridgeâ€‘Gatewayâ€‘Host (docker network inspect bridge).
  - PrÃ¼fe, ob Ollama lÃ¤uft: ollama list

- â€œModel not foundâ€ oder â€œThe model does not existâ€.
  - Der Eintrag â€œmodel name for API endpointâ€ muss exakt dem Ollamaâ€‘Tag entsprechen (z. B. llama3.1:8b). PrÃ¼fe mit ollama list oder curl http://localhost:11434/api/tags

- Was trage ich bei â€œAPI Keyâ€ ein?
  - Irgendeinen nichtâ€‘leeren String (z. B. ollama). Ollama prÃ¼ft das nicht, Dify will aber ein Feld.

- Ist .yml und .yaml dasselbe?
  - Ja. Beides ist YAML. Docker Compose akzeptiert beide. Falls der Dateiname abweicht, nutze docker compose -f datei.yaml up -d

- Responses sind langsam.
  - Kleinere Modelle nutzen.
  - In Dify die Maxâ€‘Token reduzieren.
  - Genug RAM/CPU sicherstellen.

- Kann ich ein anderes Modell nutzen?
  - Ja. In Ollama den passenden Tag ziehen (ollama pull ...), danach in Dify dasselbe Vorgehen: Model Name und model name for API endpoint identisch zum Tag.

- â€œlocalhostâ€ funktioniert nicht aus Dify.
  - Dify lÃ¤uft im Container. localhost meint dann den Container selbst, nicht deinen Host. Daher host.docker.internal (Mac/Win) bzw. Bridgeâ€‘Gateway (Linux).

- Linux: Wie finde ich die Gatewayâ€‘IP?
  - docker network inspect bridge und nach "Gateway" suchen. Typisch ist 172.17.0.1

- Portâ€‘Konflikte beim Start von Dify.
  - PrÃ¼fe, ob 3000 (Dify UI) oder weitere in der Compose belegte Ports frei sind. Passe Ports in der docker-compose.yaml an, falls nÃ¶tig, und starte neu.

- Ich habe versehentlich Knowledge/RAG aktiviert.
  - In der App die Knowledgeâ€‘Sektion entfernen bzw. sicherstellen, dass kein Datenspeicher verbunden ist. FÃ¼r Agent 1 brauchen wir kein RAG.

- Wie exportiere/importiere ich das Template?
  - In der App: Export Template -> Datei sichern. Import analog Ã¼ber Import Template.
